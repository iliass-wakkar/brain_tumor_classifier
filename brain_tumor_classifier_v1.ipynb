{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß† Brain Tumor Classification with ResNet50\n",
                "\n",
                "**Production-Grade Transfer Learning Pipeline**\n",
                "\n",
                "| Spec | Value |\n",
                "|------|-------|\n",
                "| Model | ResNet50 (ImageNet pretrained) |\n",
                "| Classes | Glioma, Meningioma, Pituitary, No Tumor |\n",
                "| Training | Two-Stage (Freeze ‚Üí Fine-tune) |\n",
                "| Server | 250GB RAM, RTX A6000 (48GB VRAM) |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 1: Environment Setup & GPU Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-12-21 18:03:42.907532: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
                        "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "2025-12-21 18:03:43.043964: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
                        "2025-12-21 18:03:43.080961: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
                        "2025-12-21 18:03:43.818922: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
                        "2025-12-21 18:03:43.819020: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
                        "2025-12-21 18:03:43.819027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Using GPU: /physical_device:GPU:0\n",
                        "\n",
                        "üì¶ NumPy Version: 1.26.4\n",
                        "üì¶ TensorFlow Version: 2.10.1\n",
                        "name, memory.total [MiB], memory.free [MiB]\n",
                        "NVIDIA RTX A6000, 49140 MiB, 48666 MiB\n",
                        "NVIDIA RTX A6000, 49140 MiB, 48666 MiB\n"
                    ]
                }
            ],
            "source": [
                "# Install dependencies (NumPy <2.0 for TensorFlow compatibility)\n",
                "!pip install -q \"numpy<2.0\" kaggle tensorflow pandas matplotlib seaborn scikit-learn\n",
                "\n",
                "import os\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "\n",
                "# Configure single GPU (avoid multi-GPU shared memory issues)\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "if gpus:\n",
                "    try:\n",
                "        # Use only the first GPU\n",
                "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
                "        # Enable memory growth to avoid allocating all VRAM at once\n",
                "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
                "        print(f\"‚úÖ Using GPU: {gpus[0].name}\")\n",
                "    except RuntimeError as e:\n",
                "        print(f\"‚ö†Ô∏è GPU config error: {e}\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No GPU detected. Training will be slow on CPU.\")\n",
                "\n",
                "print(f\"\\nüì¶ NumPy Version: {np.__version__}\")\n",
                "print(f\"üì¶ TensorFlow Version: {tf.__version__}\")\n",
                "\n",
                "# Show GPU info\n",
                "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 2: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ All libraries imported successfully!\n"
                    ]
                }
            ],
            "source": [
                "import glob\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.applications import ResNet50\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "\n",
                "print(\"‚úÖ All libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÅ Current Working Directory: /home/jovyan/work\n",
                        "\n",
                        "============================================================\n",
                        "üìÇ FILES & FOLDERS IN THIS DIRECTORY:\n",
                        "============================================================\n",
                        "üìÅ .ipynb_checkpoints/  (0 items)\n",
                        "üìÅ cancer modele/  (6 items)\n",
                        "üìÅ real_fake_ai_modele_project/  (14 items)\n",
                        "üìÅ text to voice modele/  (2 items)\n",
                        "\n",
                        "============================================================\n",
                        "‚ùå kaggle.json NOT FOUND - Please upload it to this folder!\n",
                        "‚è≥ brain_tumor_dataset/ NOT FOUND - Will download from Kaggle.\n"
                    ]
                }
            ],
            "source": [
                "# Show current working directory and its contents (shallow listing)\n",
                "import os\n",
                "\n",
                "print(f\"üìÅ Current Working Directory: {os.getcwd()}\")\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"üìÇ FILES & FOLDERS IN THIS DIRECTORY:\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for item in sorted(os.listdir('.')):\n",
                "    full_path = os.path.join('.', item)\n",
                "    if os.path.isdir(full_path):\n",
                "        # Count items in directory (but don't go deep)\n",
                "        try:\n",
                "            num_items = len(os.listdir(full_path))\n",
                "            print(f\"üìÅ {item}/  ({num_items} items)\")\n",
                "        except:\n",
                "            print(f\"üìÅ {item}/\")\n",
                "    else:\n",
                "        size_kb = os.path.getsize(full_path) / 1024\n",
                "        if size_kb > 1024:\n",
                "            size_str = f\"{size_kb/1024:.1f} MB\"\n",
                "        else:\n",
                "            size_str = f\"{size_kb:.1f} KB\"\n",
                "        print(f\"üìÑ {item}  ({size_str})\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "# Check for required files\n",
                "if os.path.exists('kaggle.json'):\n",
                "    print(\"‚úÖ kaggle.json FOUND - Ready to download dataset!\")\n",
                "else:\n",
                "    print(\"‚ùå kaggle.json NOT FOUND - Please upload it to this folder!\")\n",
                "\n",
                "if os.path.exists('brain_tumor_dataset'):\n",
                "    print(\"‚úÖ brain_tumor_dataset/ FOUND - Dataset already downloaded!\")\n",
                "else:\n",
                "    print(\"‚è≥ brain_tumor_dataset/ NOT FOUND - Will download from Kaggle.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 3: Download Dataset from Kaggle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ùå ERROR: Please upload 'kaggle.json' to this folder first!\n",
                        "   Get it from: https://www.kaggle.com/settings ‚Üí API ‚Üí Create New Token\n"
                    ]
                }
            ],
            "source": [
                "# Set Kaggle config directory to current folder\n",
                "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
                "\n",
                "# Check for kaggle.json\n",
                "if os.path.exists('kaggle.json'):\n",
                "    os.chmod('kaggle.json', 0o600)  # Secure permissions\n",
                "    print(\"‚úÖ Kaggle API token found and secured.\")\n",
                "    \n",
                "    # Download dataset if not already present\n",
                "    if not os.path.exists('brain_tumor_dataset'):\n",
                "        print(\"\\n‚¨áÔ∏è Downloading Brain Tumor MRI Dataset...\")\n",
                "        !kaggle datasets download -d masoudnickparvar/brain-tumor-mri-dataset\n",
                "        \n",
                "        print(\"\\nüìÇ Extracting...\")\n",
                "        !unzip -q brain-tumor-mri-dataset.zip -d brain_tumor_dataset\n",
                "        print(\"‚úÖ Dataset ready!\")\n",
                "    else:\n",
                "        print(\"‚úÖ Dataset already exists, skipping download.\")\n",
                "else:\n",
                "    print(\"‚ùå ERROR: Please upload 'kaggle.json' to this folder first!\")\n",
                "    print(\"   Get it from: https://www.kaggle.com/settings ‚Üí API ‚Üí Create New Token\")\n",
                "\n",
                "# Verify dataset structure\n",
                "TRAIN_DIR = 'brain_tumor_dataset/Training'\n",
                "TEST_DIR = 'brain_tumor_dataset/Testing'\n",
                "\n",
                "if os.path.exists(TRAIN_DIR):\n",
                "    print(\"\\nüìä Dataset Summary:\")\n",
                "    for split, path in [('Training', TRAIN_DIR), ('Testing', TEST_DIR)]:\n",
                "        print(f\"\\n{split}:\")\n",
                "        for class_name in sorted(os.listdir(path)):\n",
                "            class_path = os.path.join(path, class_name)\n",
                "            if os.path.isdir(class_path):\n",
                "                count = len(os.listdir(class_path))\n",
                "                print(f\"  - {class_name}: {count} images\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 4: Load Data into RAM & Create Generators"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "IMG_SIZE = (224, 224)\n",
                "BATCH_SIZE = 32\n",
                "SEED = 42\n",
                "\n",
                "# Training data generator WITH augmentation\n",
                "train_datagen = ImageDataGenerator(\n",
                "    rescale=1./255,\n",
                "    validation_split=0.2,  # 80% train, 20% validation\n",
                "    rotation_range=15,\n",
                "    width_shift_range=0.1,\n",
                "    height_shift_range=0.1,\n",
                "    zoom_range=0.1,\n",
                "    horizontal_flip=True,\n",
                "    fill_mode='nearest'\n",
                ")\n",
                "\n",
                "# Test data generator WITHOUT augmentation\n",
                "test_datagen = ImageDataGenerator(rescale=1./255)\n",
                "\n",
                "# Load Training Data (80%)\n",
                "print(\"üì• Loading Training Data...\")\n",
                "train_generator = train_datagen.flow_from_directory(\n",
                "    TRAIN_DIR,\n",
                "    target_size=IMG_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical',\n",
                "    subset='training',\n",
                "    seed=SEED,\n",
                "    shuffle=True\n",
                ")\n",
                "\n",
                "# Load Validation Data (20%)\n",
                "print(\"\\nüì• Loading Validation Data...\")\n",
                "val_generator = train_datagen.flow_from_directory(\n",
                "    TRAIN_DIR,\n",
                "    target_size=IMG_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical',\n",
                "    subset='validation',\n",
                "    seed=SEED,\n",
                "    shuffle=False\n",
                ")\n",
                "\n",
                "# Load Test Data\n",
                "print(\"\\nüì• Loading Test Data...\")\n",
                "test_generator = test_datagen.flow_from_directory(\n",
                "    TEST_DIR,\n",
                "    target_size=IMG_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical',\n",
                "    shuffle=False  # Important for evaluation\n",
                ")\n",
                "\n",
                "# Store class names for later\n",
                "CLASS_NAMES = list(train_generator.class_indices.keys())\n",
                "NUM_CLASSES = len(CLASS_NAMES)\n",
                "\n",
                "print(f\"\\n‚úÖ Classes: {CLASS_NAMES}\")\n",
                "print(f\"‚úÖ Training samples: {train_generator.samples}\")\n",
                "print(f\"‚úÖ Validation samples: {val_generator.samples}\")\n",
                "print(f\"‚úÖ Test samples: {test_generator.samples}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 5: Build ResNet50 Model Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_model(num_classes=4, freeze_backbone=True):\n",
                "    \"\"\"\n",
                "    Build ResNet50 with custom classification head.\n",
                "    \n",
                "    Args:\n",
                "        num_classes: Number of output classes\n",
                "        freeze_backbone: If True, freeze all ResNet50 layers\n",
                "    \n",
                "    Returns:\n",
                "        Compiled Keras model\n",
                "    \"\"\"\n",
                "    # Load ResNet50 backbone (pretrained on ImageNet)\n",
                "    base_model = ResNet50(\n",
                "        weights='imagenet',\n",
                "        include_top=False,  # Remove original classifier\n",
                "        input_shape=(224, 224, 3)\n",
                "    )\n",
                "    \n",
                "    # Freeze backbone if specified\n",
                "    base_model.trainable = not freeze_backbone\n",
                "    \n",
                "    # Build custom classification head\n",
                "    x = base_model.output\n",
                "    x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
                "    x = Dropout(0.5, name='dropout_1')(x)\n",
                "    x = Dense(256, activation='relu', name='fc1')(x)\n",
                "    x = Dropout(0.3, name='dropout_2')(x)\n",
                "    predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
                "    \n",
                "    # Create final model\n",
                "    model = Model(inputs=base_model.input, outputs=predictions)\n",
                "    \n",
                "    return model, base_model\n",
                "\n",
                "# Build model with frozen backbone\n",
                "model, base_model = build_model(num_classes=NUM_CLASSES, freeze_backbone=True)\n",
                "\n",
                "# Count parameters\n",
                "total_params = model.count_params()\n",
                "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
                "non_trainable_params = total_params - trainable_params\n",
                "\n",
                "print(\"üèóÔ∏è Model Architecture Summary:\")\n",
                "print(f\"   Total Parameters: {total_params:,}\")\n",
                "print(f\"   Trainable Parameters: {trainable_params:,}\")\n",
                "print(f\"   Non-trainable Parameters: {non_trainable_params:,}\")\n",
                "print(f\"\\n   Backbone Layers: {len(base_model.layers)}\")\n",
                "print(f\"   Backbone Frozen: {not base_model.trainable}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 6: Define Training Callbacks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# EarlyStopping: Stop training if validation loss doesn't improve\n",
                "early_stop = EarlyStopping(\n",
                "    monitor='val_loss',\n",
                "    patience=5,\n",
                "    restore_best_weights=True,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# ModelCheckpoint: Save best model weights\n",
                "checkpoint = ModelCheckpoint(\n",
                "    'best_brain_tumor_model.keras',\n",
                "    monitor='val_accuracy',\n",
                "    save_best_only=True,\n",
                "    mode='max',\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# ReduceLROnPlateau: Reduce learning rate when validation loss plateaus\n",
                "reduce_lr = ReduceLROnPlateau(\n",
                "    monitor='val_loss',\n",
                "    factor=0.5,\n",
                "    patience=3,\n",
                "    min_lr=1e-7,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Callbacks configured:\")\n",
                "print(\"   - EarlyStopping (patience=5, restore_best_weights=True)\")\n",
                "print(\"   - ModelCheckpoint (save best model to 'best_brain_tumor_model.keras')\")\n",
                "print(\"   - ReduceLROnPlateau (reduce LR by 0.5x if no improvement for 3 epochs)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 7: Stage 1 ‚Äî Warm-up Training (Frozen Backbone) ‚ùÑÔ∏è"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üöÄ STAGE 1: Training Custom Head (Backbone Frozen)\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Ensure backbone is frozen\n",
                "base_model.trainable = False\n",
                "\n",
                "# Compile with moderate learning rate\n",
                "model.compile(\n",
                "    optimizer=Adam(learning_rate=1e-4),\n",
                "    loss='categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "# Train Stage 1\n",
                "EPOCHS_STAGE1 = 10\n",
                "\n",
                "history_stage1 = model.fit(\n",
                "    train_generator,\n",
                "    epochs=EPOCHS_STAGE1,\n",
                "    validation_data=val_generator,\n",
                "    callbacks=[early_stop, checkpoint, reduce_lr],\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ Stage 1 Complete!\")\n",
                "print(f\"   Best Val Accuracy: {max(history_stage1.history['val_accuracy']):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 8: Stage 2 ‚Äî Fine-tuning (Unfreeze Block 5) üî•"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üî• STAGE 2: Fine-tuning (Unfreezing Top Layers)\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Unfreeze the last 20 layers (Block 5 of ResNet50)\n",
                "base_model.trainable = True\n",
                "\n",
                "# Keep first layers frozen, unfreeze only top layers\n",
                "FREEZE_UNTIL = len(base_model.layers) - 20\n",
                "for layer in base_model.layers[:FREEZE_UNTIL]:\n",
                "    layer.trainable = False\n",
                "\n",
                "# Count trainable params after unfreezing\n",
                "trainable_now = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
                "print(f\"\\nüìä Trainable parameters after unfreezing: {trainable_now:,}\")\n",
                "\n",
                "# Re-compile with VERY LOW learning rate\n",
                "model.compile(\n",
                "    optimizer=Adam(learning_rate=1e-5),  # 10x lower than Stage 1\n",
                "    loss='categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "# Reset callbacks for Stage 2\n",
                "early_stop_stage2 = EarlyStopping(\n",
                "    monitor='val_loss',\n",
                "    patience=5,\n",
                "    restore_best_weights=True,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "checkpoint_stage2 = ModelCheckpoint(\n",
                "    'best_brain_tumor_model_finetuned.keras',\n",
                "    monitor='val_accuracy',\n",
                "    save_best_only=True,\n",
                "    mode='max',\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# Train Stage 2\n",
                "EPOCHS_STAGE2 = 20\n",
                "\n",
                "history_stage2 = model.fit(\n",
                "    train_generator,\n",
                "    epochs=EPOCHS_STAGE2,\n",
                "    validation_data=val_generator,\n",
                "    callbacks=[early_stop_stage2, checkpoint_stage2, reduce_lr],\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ Stage 2 Complete!\")\n",
                "print(f\"   Best Val Accuracy: {max(history_stage2.history['val_accuracy']):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 9: Training History Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_training_history(history1, history2):\n",
                "    \"\"\"Plot combined training history from both stages.\"\"\"\n",
                "    \n",
                "    # Combine histories\n",
                "    acc = history1.history['accuracy'] + history2.history['accuracy']\n",
                "    val_acc = history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
                "    loss = history1.history['loss'] + history2.history['loss']\n",
                "    val_loss = history1.history['val_loss'] + history2.history['val_loss']\n",
                "    \n",
                "    epochs = range(1, len(acc) + 1)\n",
                "    stage1_end = len(history1.history['accuracy'])\n",
                "    \n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    # Accuracy plot\n",
                "    axes[0].plot(epochs, acc, 'b-', label='Training Accuracy', linewidth=2)\n",
                "    axes[0].plot(epochs, val_acc, 'r-', label='Validation Accuracy', linewidth=2)\n",
                "    axes[0].axvline(x=stage1_end, color='green', linestyle='--', label='Fine-tuning Start')\n",
                "    axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
                "    axes[0].set_xlabel('Epoch')\n",
                "    axes[0].set_ylabel('Accuracy')\n",
                "    axes[0].legend()\n",
                "    axes[0].grid(True, alpha=0.3)\n",
                "    \n",
                "    # Loss plot\n",
                "    axes[1].plot(epochs, loss, 'b-', label='Training Loss', linewidth=2)\n",
                "    axes[1].plot(epochs, val_loss, 'r-', label='Validation Loss', linewidth=2)\n",
                "    axes[1].axvline(x=stage1_end, color='green', linestyle='--', label='Fine-tuning Start')\n",
                "    axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
                "    axes[1].set_xlabel('Epoch')\n",
                "    axes[1].set_ylabel('Loss')\n",
                "    axes[1].legend()\n",
                "    axes[1].grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig('training_history.png', dpi=150)\n",
                "    plt.show()\n",
                "    print(\"\\nüìä Training history saved to 'training_history.png'\")\n",
                "\n",
                "plot_training_history(history_stage1, history_stage2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 10: Model Evaluation on Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üìä EVALUATING MODEL ON TEST SET\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Load best model\n",
                "from tensorflow.keras.models import load_model\n",
                "\n",
                "best_model_path = 'best_brain_tumor_model_finetuned.keras'\n",
                "if os.path.exists(best_model_path):\n",
                "    model = load_model(best_model_path)\n",
                "    print(f\"‚úÖ Loaded best model from: {best_model_path}\")\n",
                "\n",
                "# Evaluate on test set\n",
                "test_loss, test_accuracy = model.evaluate(test_generator, verbose=0)\n",
                "print(f\"\\nüéØ Test Loss: {test_loss:.4f}\")\n",
                "print(f\"üéØ Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
                "\n",
                "# Generate predictions\n",
                "print(\"\\n‚è≥ Generating predictions...\")\n",
                "test_generator.reset()\n",
                "predictions = model.predict(test_generator, verbose=1)\n",
                "y_pred = np.argmax(predictions, axis=1)\n",
                "y_true = test_generator.classes\n",
                "\n",
                "# Classification Report\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"üìã CLASSIFICATION REPORT\")\n",
                "print(\"=\" * 50)\n",
                "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix Visualization\n",
                "print(\"\\nüìä CONFUSION MATRIX\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(\n",
                "    cm, \n",
                "    annot=True, \n",
                "    fmt='d', \n",
                "    cmap='Blues',\n",
                "    xticklabels=CLASS_NAMES,\n",
                "    yticklabels=CLASS_NAMES,\n",
                "    annot_kws={'size': 14}\n",
                ")\n",
                "plt.title('Confusion Matrix - Brain Tumor Classification', fontsize=14, fontweight='bold')\n",
                "plt.xlabel('Predicted Label', fontsize=12)\n",
                "plt.ylabel('True Label', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.savefig('confusion_matrix.png', dpi=150)\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüìä Confusion matrix saved to 'confusion_matrix.png'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 11: Save Final Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final model\n",
                "FINAL_MODEL_PATH = 'brain_tumor_resnet50_final.keras'\n",
                "model.save(FINAL_MODEL_PATH)\n",
                "print(f\"\\n‚úÖ Final model saved to: {FINAL_MODEL_PATH}\")\n",
                "\n",
                "# Summary of saved files\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"üìÅ SAVED FILES SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "saved_files = [\n",
                "    ('best_brain_tumor_model.keras', 'Best model from Stage 1'),\n",
                "    ('best_brain_tumor_model_finetuned.keras', 'Best model from Stage 2 (Fine-tuned)'),\n",
                "    ('brain_tumor_resnet50_final.keras', 'Final trained model'),\n",
                "    ('training_history.png', 'Training curves visualization'),\n",
                "    ('confusion_matrix.png', 'Confusion matrix heatmap')\n",
                "]\n",
                "\n",
                "for filename, description in saved_files:\n",
                "    if os.path.exists(filename):\n",
                "        size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
                "        print(f\"  ‚úÖ {filename} ({size_mb:.2f} MB) - {description}\")\n",
                "    else:\n",
                "        print(f\"  ‚ùå {filename} - Not found\")\n",
                "\n",
                "print(\"\\nüéâ Training Complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üìä Quick Inference Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on a few random images from test set\n",
                "import random\n",
                "from tensorflow.keras.preprocessing import image\n",
                "\n",
                "def predict_single_image(img_path, model, class_names):\n",
                "    \"\"\"Predict class for a single image.\"\"\"\n",
                "    img = image.load_img(img_path, target_size=(224, 224))\n",
                "    img_array = image.img_to_array(img) / 255.0\n",
                "    img_array = np.expand_dims(img_array, axis=0)\n",
                "    \n",
                "    pred = model.predict(img_array, verbose=0)\n",
                "    pred_class = class_names[np.argmax(pred)]\n",
                "    confidence = np.max(pred) * 100\n",
                "    \n",
                "    return pred_class, confidence, img\n",
                "\n",
                "# Get random test images\n",
                "test_images = []\n",
                "for class_name in CLASS_NAMES:\n",
                "    class_dir = os.path.join(TEST_DIR, class_name)\n",
                "    images = os.listdir(class_dir)[:2]  # Get 2 images per class\n",
                "    for img_name in images:\n",
                "        test_images.append((os.path.join(class_dir, img_name), class_name))\n",
                "\n",
                "# Show predictions\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for idx, (img_path, true_label) in enumerate(test_images[:8]):\n",
                "    pred_class, confidence, img = predict_single_image(img_path, model, CLASS_NAMES)\n",
                "    \n",
                "    axes[idx].imshow(img)\n",
                "    color = 'green' if pred_class == true_label else 'red'\n",
                "    axes[idx].set_title(f\"True: {true_label}\\nPred: {pred_class} ({confidence:.1f}%)\", \n",
                "                        color=color, fontsize=10)\n",
                "    axes[idx].axis('off')\n",
                "\n",
                "plt.suptitle('Sample Predictions from Test Set', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('sample_predictions.png', dpi=150)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
